{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6D_AlcYJS2-t",
        "sj_y5uinTK1G",
        "JHO6-E-u80qZ",
        "STITAK4o9LS1",
        "Dbgts-CbqsJk"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreaTribotti/project/blob/main/Furlani_Serfilippi_Tribotti_ml_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to load the dataset"
      ],
      "metadata": {
        "id": "GtsSBn9AfKXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"datasets==3.6.0\""
      ],
      "metadata": {
        "id": "_2LCie5Mac7i",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gdown\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import math\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "lI3gCsQ7jR-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "id": "pZu5RpW7THzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Preparation**"
      ],
      "metadata": {
        "id": "0xZEs6JcQ3Hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOU MUST NOT CHANGE THIS CELL! ###\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "full_dataset = load_dataset(\"skeskinen/TinyStories-GPT4\", split=\"train\")\n",
        "full_dataset = full_dataset.remove_columns([c for c in full_dataset.column_names if c not in [\"story\", \"features\"]])\n",
        "assert len(full_dataset) == 2745100\n",
        "\n",
        "splits = full_dataset.train_test_split(test_size=10000, seed=42, shuffle=True)\n",
        "\n",
        "train_dataset = splits[\"train\"]\n",
        "test_dataset  = splits[\"test\"]\n",
        "\n",
        "assert len(train_dataset) == 2735100\n",
        "assert len(test_dataset)  == 10000\n",
        "\n",
        "assert train_dataset[0][\"story\"][:33] == \"One day, a little girl named Lily\"\n",
        "assert train_dataset[0][\"features\"] == [\"Dialogue\", \"Conflict\"]"
      ],
      "metadata": {
        "id": "ErtziEODYM6U",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "random.seed(42)\n",
        "n=250000 #number of samples\n",
        "TAGS=[\"BadEnding\", \"Conflict\", \"Dialogue\", \"Foreshadowing\", \"MoralValue\", \"Twist\"]\n",
        "\n",
        "subset_indices = random.sample(range(len(train_dataset)), n)\n",
        "\n",
        "# Create the subset that will be used to build the vocabulary\n",
        "vocab_subset = train_dataset.select(subset_indices)\n",
        "\n",
        "# Split the training set into train_subset_new and validation_subset\n",
        "random.seed(1)\n",
        "subset_indices_1 = random.sample(range(len(train_dataset)), n)\n",
        "train_subset = train_dataset.select(subset_indices_1)\n",
        "split = train_subset.train_test_split(test_size=0.2, seed=1)\n",
        "train_subset_new = split[\"train\"]\n",
        "validation_subset = split[\"test\"]"
      ],
      "metadata": {
        "id": "ZV6v5ApR2Lx3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we print the first example of the train dataset\n",
        "\n",
        "from pprint import pprint\n",
        "pprint(train_dataset[0])"
      ],
      "metadata": {
        "id": "G__WK0ycbIvh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def encode_tags_fast(tag_matrix, tag_list):\n",
        "    \"\"\"\n",
        "    tag_matrix: list of lists of tags for each story, shape [n, ?]\n",
        "    tag_list: list of all possible tags\n",
        "\n",
        "    Returns: multi-hot matrix [n, len(tag_list)]\n",
        "    \"\"\"\n",
        "    n = len(tag_matrix)\n",
        "    num_tags = len(tag_list)\n",
        "    tags_multi_hot = np.zeros((n, num_tags), dtype=np.float32)\n",
        "\n",
        "    for j, tag in enumerate(tag_list):\n",
        "        tags_multi_hot[:, j] = np.array([tag in story_tags for story_tags in tag_matrix], dtype=np.float32)\n",
        "\n",
        "    return tags_multi_hot\n"
      ],
      "metadata": {
        "id": "t4QIbA9NS5w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **First model**\n",
        "A multilabel logistic regression model, used to perform independent binary classification for each of the six labels"
      ],
      "metadata": {
        "id": "Xb5SCeg0L5vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract texts and multi-hot encoded tags\n",
        "X_text = [item['story'] for item in train_subset_new]\n",
        "y_train_multi = encode_tags_fast([item['features'] for item in train_subset_new], TAGS)\n",
        "\n",
        "X_test_text = [item['story'] for item in test_dataset]\n",
        "y_test_multi = encode_tags_fast([item['features'] for item in test_dataset], TAGS)\n",
        "\n",
        "# Convert text into numerical features using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))  # unigram + bigram\n",
        "X_train_tfidf = vectorizer.fit_transform(X_text)\n",
        "X_test_tfidf = vectorizer.transform(X_test_text)\n",
        "\n",
        "# Logistic Regression with OneVsRestClassifier\n",
        "base_clf = LogisticRegression(max_iter=10000,  class_weight=\"balanced\")\n",
        "model = OneVsRestClassifier(base_clf)\n"
      ],
      "metadata": {
        "id": "zPZOgo8lfkpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First model training and prediction"
      ],
      "metadata": {
        "id": "Pr0UZcVVRzEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_tfidf, y_train_multi)\n",
        "\n",
        "# Predictions\n",
        "y_pred_multi = model.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "C1Jo-R7XRx4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First model evaluation"
      ],
      "metadata": {
        "id": "GErIOWamRnch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies = [(y_pred_multi[:, i] == y_test_multi[:, i]).mean() for i in range(len(TAGS))]\n",
        "for i, acc in enumerate(accuracies):\n",
        "    print(f\"Accuracy tag {TAGS[i]}: {acc*100:.2f}%\")\n",
        "\n",
        "#Precision, Recall and F1 per tag\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "for i in range(len(TAGS)):\n",
        "    p = precision_score(y_test_multi[:, i], y_pred_multi[:, i], zero_division=0)\n",
        "    r = recall_score(y_test_multi[:, i], y_pred_multi[:, i], zero_division=0)\n",
        "    f1s = f1_score(y_test_multi[:, i], y_pred_multi[:, i], zero_division=0)\n",
        "\n",
        "    precisions.append(p)\n",
        "    recalls.append(r)\n",
        "    f1_scores.append(f1s)\n",
        "\n",
        "    print(f\"Tag {TAGS[i]} -> Precision: {p:.4f}, Recall: {r:.4f}, F1: {f1s:.4f}\")\n",
        "\n",
        "# Averages\n",
        "print(\"\\n--- AVARAGES ---\")\n",
        "print(f\"Average Accuracy: {np.mean(accuracies)*100:.2f}%\")\n",
        "print(f\"Average Precision: {np.mean(precisions):.4f}\")\n",
        "print(f\"Average Recall: {np.mean(recalls):.4f}\")\n",
        "print(f\"Average F1: {np.mean(f1_scores):.4f}\")\n",
        "print(f\"Average Accuracy: {np.mean(accuracies)*100:.2f}%\")\n",
        "print(f\"Average F1: {np.mean(f1_scores):.4f}\")"
      ],
      "metadata": {
        "id": "XZZkrJsxRkqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Second model**\n",
        "A Transformer-based model for multilabel text classification"
      ],
      "metadata": {
        "id": "vTykFhBqMWo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build vocabulary"
      ],
      "metadata": {
        "id": "tGDneMJWSK9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Pattern:  words, numbers and punctuation characters\n",
        "pattern = r\"\\w+|[^\\w\\s]\"\n",
        "\n",
        "# Loop over the first n stories in the subset to extract tokens which will be used to build the vocabulary\n",
        "vocab_set = set()\n",
        "\n",
        "for i in range(n):\n",
        "    tokens = re.findall(pattern, vocab_subset[i]['story'].lower())\n",
        "    vocab_set.update(tokens)\n",
        "\n",
        "# Special tokens are defined to manage padding and unknown words\n",
        "PAD_TOKEN = '[PAD]'\n",
        "UNK_TOKEN = '[UNK]'\n",
        "\n",
        "# Create the vocabulary\n",
        "vocab_set = [PAD_TOKEN, UNK_TOKEN] + list(sorted(vocab_set))\n",
        "token_to_index = {token: idx for idx, token in enumerate(vocab_set)}\n",
        "\n",
        "print(\"Vocabulary:\", token_to_index)\n",
        "print(\"Number of tokens:\", len(token_to_index))"
      ],
      "metadata": {
        "id": "gIY-bbb7sz2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization, Encoding and Padding of stories"
      ],
      "metadata": {
        "id": "7ro4ewJGPRig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Number of tokens per story\n",
        "n_tokens = 256\n",
        "\n",
        "def encode(x):\n",
        "    \"\"\"\n",
        "    the function encode convert a list of tokens to their corresponding indices in the vocabulary.\n",
        "    If a token is not in the vocabulary, it is replaced by the UNK_TOKEN\n",
        "    \"\"\"\n",
        "    return [token_to_index.get(token, token_to_index[UNK_TOKEN])  for token in x]\n",
        "\n",
        "\n",
        "def truncate_and_pad(sequence, n_tokens=256):\n",
        "    \"\"\"\n",
        "    Truncate a sequence to n_tokens.\n",
        "    Add padding tokens if the sequence is shorter.\n",
        "    \"\"\"\n",
        "    if len(sequence) < n_tokens:\n",
        "        sequence = sequence + [PAD_TOKEN] * (n_tokens - len(sequence))\n",
        "    else:\n",
        "        sequence = sequence[:n_tokens]\n",
        "    return sequence\n",
        "\n",
        "\n",
        "def encode_multiple(data):\n",
        "    n = len(data)\n",
        "    X = []\n",
        "    for i in range(n):\n",
        "        words = re.findall(pattern, data[i]['story'].lower())\n",
        "        X.append(encode(truncate_and_pad(words, n_tokens)))\n",
        "    return X"
      ],
      "metadata": {
        "id": "b5zvevpmiLKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding of tags"
      ],
      "metadata": {
        "id": "A1Qmjg9lQiLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(data):\n",
        "    encoded_data = encode_multiple(data)\n",
        "    encoded_tag = encode_tags_fast(data[:]['features'], TAGS)\n",
        "    return TensorDataset(torch.tensor(encoded_data), torch.tensor(encoded_tag))\n",
        "\n",
        "train_dataset = build_dataset(train_subset_new)\n",
        "val_dataset = build_dataset(validation_subset)\n",
        "test_dataset = build_dataset(test_dataset)"
      ],
      "metadata": {
        "id": "0IQnaL-SLY_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second model definition"
      ],
      "metadata": {
        "id": "UvCqzrwGST64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters\n",
        "VOCAB_SIZE = len(token_to_index)\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 128\n",
        "NUM_HEADS = 4\n",
        "NUM_TAGS = 6\n",
        "batch_size=128\n",
        "\n",
        "\n",
        "encoder_layer = nn.TransformerEncoderLayer(\n",
        "    d_model=EMBEDDING_DIM,\n",
        "    nhead=NUM_HEADS,\n",
        "    dim_feedforward=HIDDEN_DIM,\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "\n",
        "class LearnedPositionalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Adds a trainable position vector to each token embedding.\n",
        "    Expects input shape [B, L, D] (batch_first).\n",
        "    \"\"\"\n",
        "    def __init__(self, max_length: int, dim: int):\n",
        "        super().__init__()\n",
        "        self.pos_emb = nn.Embedding(max_length, dim)\n",
        "\n",
        "    def forward(self, x):                   # x: [B, L, D]\n",
        "        L = x.size(1)\n",
        "        pos = torch.arange(L, device=x.device)          # [L]\n",
        "        pos = pos.unsqueeze(0)                          # [1, L]\n",
        "        return x + self.pos_emb(pos)                    # broadcast add\n",
        "\n",
        "class MaxPooling(nn.Module):\n",
        "    def forward(self, x):   # x: [B, L, D]\n",
        "        return x.max(dim=1).values  # output: [B, D]\n",
        "\n",
        "\n",
        "# Full model\n",
        "model = nn.Sequential(\n",
        "    nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM, padding_idx=token_to_index[PAD_TOKEN]),\n",
        "    LearnedPositionalEmbedding(n_tokens, EMBEDDING_DIM),\n",
        "    nn.TransformerEncoder(encoder_layer, num_layers=1),\n",
        "    MaxPooling(),\n",
        "    nn.Linear(EMBEDDING_DIM, NUM_TAGS),\n",
        ")\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")"
      ],
      "metadata": {
        "id": "gAxiHmP_zIcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and evaluation functions"
      ],
      "metadata": {
        "id": "NgT3kZ98SffR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define the training function\n",
        "\"\"\"\n",
        "\n",
        "def train(model, train_dataset, val_dataset, optimizer, batch_size, epochs, device):\n",
        "    model.to(device)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss() # Multi-label loss\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      print(f\"--- Epoch {epoch+1} ---\")\n",
        "      model.train()\n",
        "      losses = []\n",
        "      for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "          data, target = data.to(device), target.to(device).float()\n",
        "          optimizer.zero_grad()\n",
        "          output = model(data)\n",
        "          loss = loss_fn(output, target)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          losses.append(loss.item())\n",
        "      training_loss = np.mean(losses)\n",
        "\n",
        "      # VALIDATION\n",
        "      validation_loss, val_accuracies = evaluate(model, val_loader, device)\n",
        "      avg_val_acc = sum(val_accuracies) / len(val_accuracies)\n",
        "\n",
        "      # PRINT\n",
        "      print(f\"Training loss = {training_loss:.4f} | Validation loss = {validation_loss:.4f}\")\n",
        "      print()\n",
        "\n",
        "def evaluate(model, loader, device, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the validation set.\n",
        "    Return the avarage loss and accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device).float()\n",
        "            output = model(data)\n",
        "            total_loss += loss_fn(output, target).item()\n",
        "\n",
        "            preds = (torch.sigmoid(output) > threshold).int()\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_targets.append(target.cpu().int())\n",
        "\n",
        "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
        "    all_targets = torch.cat(all_targets, dim=0).numpy()\n",
        "\n",
        "    # Accuracy, precision, recall, F1 score per tag\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for i in range(all_targets.shape[1]):\n",
        "        acc_i = (all_preds[:, i] == all_targets[:, i]).mean()\n",
        "        accuracies.append(acc_i)\n",
        "\n",
        "        p = precision_score(all_targets[:, i], all_preds[:, i], zero_division=0)\n",
        "        r = recall_score(all_targets[:, i], all_preds[:, i], zero_division=0)\n",
        "        f = f1_score(all_targets[:, i], all_preds[:, i], zero_division=0)\n",
        "\n",
        "        precisions.append(p)\n",
        "        recalls.append(r)\n",
        "        f1_scores.append(f)\n",
        "\n",
        "        print(f\"Label {i}: Accuracy={acc_i*100:.2f}%, Precision={p:.3f}, Recall={r:.3f}, F1={f:.3f}\")\n",
        "\n",
        "    # Avarage accuracy, avarage precison, avarage recall and avarage F1 score\n",
        "    avg_acc = sum(accuracies) / len(accuracies)\n",
        "    avg_prec = sum(precisions) / len(precisions)\n",
        "    avg_rec = sum(recalls) / len(recalls)\n",
        "    avg_f1 = sum(f1_scores) / len(f1_scores)\n",
        "\n",
        "    print(f\"Average Accuracy: {avg_acc*100:.2f}%\")\n",
        "    print(f\"Average Precision: {avg_prec:.3f}\")\n",
        "    print(f\"Average Recall: {avg_rec:.3f}\")\n",
        "    print(f\"Average F1: {avg_f1:.3f}\")\n",
        "    return total_loss / len(loader), accuracies"
      ],
      "metadata": {
        "id": "ZSBvF7ggqt9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second model training"
      ],
      "metadata": {
        "id": "6D_AlcYJS2-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following two cells are commented out because, to speed up training, we have already trained the model and uploaded the weights to Drive."
      ],
      "metadata": {
        "id": "d82WjXj1pMLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "# train(model, train_dataset, val_dataset, optimizer, batch_size=128, epochs=5, device='cuda')"
      ],
      "metadata": {
        "id": "BTe7689d8TCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# save_path = \"/content/drive/MyDrive/Colab_Notebooks\"\n",
        "\n",
        "# torch.save(model.state_dict(), f\"{save_path}/Pesi_Sara.pth\")"
      ],
      "metadata": {
        "id": "sSfd3F59B27Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We download the weights of the trained model from Drive."
      ],
      "metadata": {
        "id": "ChiOL4fhsjNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gdown\n",
        "\n",
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "url = \"https://drive.google.com/file/d/1CzcwOsdImZBsWGNP5-sRlFK4GoFVqjO9/view?usp=drive_link\"\n",
        "\n",
        "downloaded_file = \"second_model.zip\"\n",
        "\n",
        "# --- Download the file ---\n",
        "gdown.download(f\"https://drive.google.com/uc?id={\"1CzcwOsdImZBsWGNP5-sRlFK4GoFVqjO9\"}\", downloaded_file, quiet=False)\n",
        "\n",
        "# --- extract the ZIP file ---\n",
        "output_folder = \"second_model\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "with zipfile.ZipFile(downloaded_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_folder)\n",
        "\n",
        "subfolder = os.path.join(\"second_model\", \"pesi_second_model\")\n",
        "files = os.listdir(subfolder)\n",
        "\n",
        "print(f\"Download completed and folder extracted to '{output_folder}'\")\n",
        "print(\"Contents of second_model:\", os.listdir(output_folder))\n",
        "print(\"Contents of pesi_second_model:\",files)"
      ],
      "metadata": {
        "id": "2rZDrsm-CNHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"second_model/pesi_second_model/Pesi_Sara.pth\", map_location=torch.device(\"cpu\")))"
      ],
      "metadata": {
        "id": "Ulo2d1n1CUK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second model evaluation"
      ],
      "metadata": {
        "id": "sj_y5uinTK1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "print(\"--- Test on unseen data ---\")\n",
        "print()\n",
        "test_loss, test_accuracies = evaluate(model, test_loader, device='cuda')\n",
        "\n",
        "avg_test_acc = sum(test_accuracies) / len(test_accuracies)\n",
        "print(f\"Test Loss = {test_loss:.4f}\")\n",
        "print(f\"Test Average Accuracy = {avg_test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "jvKP8qbMP_fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Third model**\n",
        "We perform fine-tuning of the DistilBERT model for text classification."
      ],
      "metadata": {
        "id": "twuPlBm08nYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## distilbert-base-uncased model"
      ],
      "metadata": {
        "id": "JHO6-E-u80qZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Load the pre-trained model and the tokenizer\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6, problem_type=\"multi_label_classification\")\n"
      ],
      "metadata": {
        "id": "NT9K__eJHRTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's simulate a batch of 1 sequence with 256 tokens\n",
        "batch_size = 1\n",
        "sequence_length = 256\n",
        "\n",
        "# Input tensors\n",
        "input_ids = torch.randint(0, 30522, (batch_size, sequence_length), dtype=torch.long)   # 30522 = size of DistilBERT's vocabulary\n",
        "attention_mask = torch.ones((batch_size, sequence_length), dtype=torch.long)\n",
        "\n",
        "# summary\n",
        "summary(\n",
        "    model,\n",
        "    input_data=(input_ids, attention_mask),\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    device='cpu'\n",
        "    )"
      ],
      "metadata": {
        "id": "6lqtnQCHJfB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")"
      ],
      "metadata": {
        "id": "LKLd-ELIV8Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "STITAK4o9LS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"story\"],\n",
        "        padding=\"max_length\",   # uniforma le lunghezze\n",
        "        truncation=True,        # taglia se troppo lunga\n",
        "        max_length=256          # lunghezza massima 512\n",
        "    )"
      ],
      "metadata": {
        "id": "PUJqpp94MW7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell is commented out because, to speed up workflow, we have already tokanized the data and uploaded the tokanized sets to Drive."
      ],
      "metadata": {
        "id": "QKW6eTXbqq-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We tokenize the data\n",
        "# tokenized_train = train_subset_new.map(tokenize_function, batched=True)\n",
        "# tokenized_val   = validation_subset.map(tokenize_function, batched=True)\n",
        "# tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# We set the format to Torch\n",
        "# tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "# tokenized_val.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "# tokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# save_path = \"/content/drive/MyDrive/Colab_Notebooks/tokenized_dataset\"\n",
        "\n",
        "# tokenized_train.save_to_disk(f\"{save_path}/train\")\n",
        "# tokenized_val.save_to_disk(f\"{save_path}/val\")\n",
        "# tokenized_test.save_to_disk(f\"{save_path}/test\")"
      ],
      "metadata": {
        "id": "xbAMvQEUqfST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We download the tokanized datasets from Drive."
      ],
      "metadata": {
        "id": "t9p1m1xqsbay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://drive.google.com/file/d/1oovVts5r5Vav-Q1vb6-TorFP5diyuymu/view?usp=drive_link\"\n",
        "\n",
        "downloaded_file = \"folder_tokenized_dataset.zip\"\n",
        "\n",
        "# --- Download the file ---\n",
        "gdown.download(f\"https://drive.google.com/uc?id={\"1oovVts5r5Vav-Q1vb6-TorFP5diyuymu\"}\", downloaded_file, quiet=False)\n",
        "\n",
        "# --- extract the ZIP file ---\n",
        "output_folder = \"folder_tokenized_dataset\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "with zipfile.ZipFile(downloaded_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_folder)\n",
        "\n",
        "subfolder = os.path.join(\"folder_tokenized_dataset\", \"tokenized_dataset\")\n",
        "files = os.listdir(subfolder)\n",
        "\n",
        "print(f\"Download completed and folder extracted to '{output_folder}'\")\n",
        "print(\"Contents of folder_tokenized_dataset:\", os.listdir(output_folder))\n",
        "print(\"Contents of tokenized_dataset:\",files)"
      ],
      "metadata": {
        "id": "FMAnKq2xU7Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for subset in [\"train\", \"val\", \"test\"]:\n",
        "    folder = os.path.join(\"folder_tokenized_dataset\", \"tokenized_dataset\", subset)\n",
        "    print(f\"{subset} contains: {os.listdir(folder)}\")"
      ],
      "metadata": {
        "id": "wC4Kxpnbb6d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "base_folder = os.path.join(\"folder_tokenized_dataset\", \"tokenized_dataset\")\n",
        "\n",
        "train_path = os.path.join(base_folder, \"train\")\n",
        "val_path = os.path.join(base_folder, \"val\")\n",
        "test_path = os.path.join(base_folder, \"test\")\n",
        "\n",
        "# --- loading the tokenized datasets ---\n",
        "tokenized_train = load_from_disk(train_path)\n",
        "tokenized_val = load_from_disk(val_path)\n",
        "tokenized_test = load_from_disk(test_path)"
      ],
      "metadata": {
        "id": "ARCDwDhQdVI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of examples in tokenized_train:\", len(tokenized_train))\n",
        "print(\"Number of examples in tokenized_val:\", len(tokenized_val))\n",
        "\n",
        "# Here we print the first example of the training subset and its corresponding tokenized version\n",
        "pprint(train_subset_new[0])\n",
        "pprint(tokenized_train[0])"
      ],
      "metadata": {
        "id": "BHTzyITnMnQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We convert story tags into a binary vector where 1 indicates the tag is present and 0 means absent."
      ],
      "metadata": {
        "id": "ackUH86Agom9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TAGS = [\"BadEnding\", \"Conflict\", \"Dialogue\", \"Foreshadowing\", \"MoralValue\", \"Twist\"]\n",
        "\n",
        "# We create a mapping from each tag to a unique index\n",
        "tag2id = {tag: i for i, tag in enumerate(TAGS)}\n",
        "print(tag2id)\n",
        "\n",
        "def encode_labels(features):\n",
        "    labels = [0] * len(TAGS)\n",
        "    for tag in features:\n",
        "        if tag in tag2id:\n",
        "            labels[tag2id[tag]] = 1\n",
        "    return torch.tensor(labels, dtype=torch.float)"
      ],
      "metadata": {
        "id": "JN2xThWdeuKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This following cell is commented out because, in order to speed up the workflow, the encoded tag vectors have already been pre-computed and uploaded to Drive."
      ],
      "metadata": {
        "id": "caNzd_76ppoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_train = tokenized_train.map(lambda x: {\"labels\": encode_labels(x[\"features\"])})\n",
        "# tokenized_val   = tokenized_val.map(lambda x: {\"labels\": encode_labels(x[\"features\"])})\n",
        "# tokenized_test   = tokenized_test.map(lambda x: {\"labels\": encode_labels(x[\"features\"])})\n",
        "\n",
        "# We set the format to Torch\n",
        "# tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "# tokenized_val.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# save_path = \"/content/drive/MyDrive/Colab_Notebooks/labelled_dataset\"\n",
        "\n",
        "# tokenized_train.save_to_disk(f\"{save_path}/lab_train\")\n",
        "# tokenized_val.save_to_disk(f\"{save_path}/lab_val\")\n",
        "# tokenized_test.save_to_disk(f\"{save_path}/lab_test\")"
      ],
      "metadata": {
        "id": "gaTwYOeHq-oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We download the encoded tag vectors from Drive."
      ],
      "metadata": {
        "id": "4BxcpzudsS2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://drive.google.com/file/d/1ZsvBpvq9trbOLqMuxM7VECMLG1k_hbe3/view?usp=drive_link\"\n",
        "\n",
        "downloaded_file = \"folder_labelled.zip\"\n",
        "\n",
        "# --- Download the file ---\n",
        "gdown.download(f\"https://drive.google.com/uc?id={\"1ZsvBpvq9trbOLqMuxM7VECMLG1k_hbe3\"}\", downloaded_file, quiet=False)\n",
        "\n",
        "# --- extract the ZIP file ---\n",
        "output_folder = \"folder_labelled\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "with zipfile.ZipFile(downloaded_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_folder)\n",
        "\n",
        "subfolder = os.path.join(\"folder_labelled\", \"labelled_dataset\")\n",
        "files = os.listdir(subfolder)\n",
        "\n",
        "print(f\"Download completed and folder extracted to '{output_folder}'\")\n",
        "print(\"Contents of folder_labelled:\", os.listdir(output_folder))\n",
        "print(\"Contents of labelled_dataset:\",files)"
      ],
      "metadata": {
        "id": "F_UM8Wj6ar-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for subset in [\"lab_train\", \"lab_val\", \"lab_test\"]:\n",
        "    folder = os.path.join(\"folder_labelled\", \"labelled_dataset\", subset)\n",
        "    print(f\"{subset} contains: {os.listdir(folder)}\")"
      ],
      "metadata": {
        "id": "_fMDNCZiakNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_folder = os.path.join(\"folder_labelled\", \"labelled_dataset\")\n",
        "\n",
        "train_path_lab = os.path.join(base_folder, \"lab_train\")\n",
        "val_path_lab = os.path.join(base_folder, \"lab_val\")\n",
        "test_path_lab = os.path.join(base_folder, \"lab_test\")\n",
        "\n",
        "# --- loading the tokenized datasets ---\n",
        "tokenized_train = load_from_disk(train_path_lab)\n",
        "tokenized_val = load_from_disk(val_path_lab)\n",
        "tokenized_test = load_from_disk(test_path_lab)"
      ],
      "metadata": {
        "id": "cb005L1vdXRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we print how the first example of the tokenized_train change\n",
        "pprint(tokenized_train[0])"
      ],
      "metadata": {
        "id": "mITeIbuxhn7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training and test evaluation"
      ],
      "metadata": {
        "id": "of4tLO1ijnAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_dataset, val_dataset, optimizer, batch_size, epochs, device):\n",
        "    model.to(device)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()  # multi-label classification\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"--- Epoch {epoch+1} ---\")\n",
        "        model.train()\n",
        "        losses = []\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "          # prepare data and labels\n",
        "          inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
        "          labels = batch[\"labels\"].to(device).float()\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(**inputs).logits  # HuggingFace models return an object, we need .logits\n",
        "          loss = loss_fn(outputs, labels)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          losses.append(loss.item())\n",
        "\n",
        "        training_loss = np.mean(losses)\n",
        "\n",
        "        # VALIDATION\n",
        "        validation_loss, val_accuracies = evaluate(model, val_loader, device)\n",
        "        avg_val_acc = sum(val_accuracies) / len(val_accuracies)\n",
        "\n",
        "        # PRINT\n",
        "        print(f\"Training loss = {training_loss:.4f} | Validation loss = {validation_loss:.4f}\")\n",
        "\n",
        "def evaluate(model, loader, device, threshold=0.3):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the validation set.\n",
        "    Return the average loss and accuracy per label.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch in loader:\n",
        "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
        "        labels = batch[\"labels\"].to(device).float()\n",
        "\n",
        "        outputs = model(**inputs).logits\n",
        "        total_loss += loss_fn(outputs, labels).item()\n",
        "\n",
        "        preds = (torch.sigmoid(outputs) > threshold).int()\n",
        "        all_preds.append(preds.cpu())\n",
        "        all_targets.append(labels.cpu().int())\n",
        "\n",
        "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
        "    all_targets = torch.cat(all_targets, dim=0).numpy()\n",
        "\n",
        "    # Accuracy, precision, recall, F1 score per tag\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for i in range(all_targets.shape[1]):\n",
        "      acc_i = (all_preds[:, i] == all_targets[:, i]).mean()\n",
        "      accuracies.append(acc_i)\n",
        "\n",
        "      p = precision_score(all_targets[:, i], all_preds[:, i], zero_division=0)\n",
        "      r = recall_score(all_targets[:, i], all_preds[:, i], zero_division=0)\n",
        "      f = f1_score(all_targets[:, i], all_preds[:, i], zero_division=0)\n",
        "\n",
        "      precisions.append(p)\n",
        "      recalls.append(r)\n",
        "      f1_scores.append(f)\n",
        "\n",
        "      print(f\"Label {i}: Accuracy={acc_i*100:.2f}%, Precision={p:.3f}, Recall={r:.3f}, F1={f:.3f}\")\n",
        "\n",
        "    # Average metrics\n",
        "    avg_acc = sum(accuracies) / len(accuracies)\n",
        "    avg_prec = sum(precisions) / len(precisions)\n",
        "    avg_rec = sum(recalls) / len(recalls)\n",
        "    avg_f1 = sum(f1_scores) / len(f1_scores)\n",
        "\n",
        "    print(f\"Average Accuracy: {avg_acc*100:.2f}%\")\n",
        "    print(f\"Average Precision: {avg_prec:.3f}, Average Recall: {avg_rec:.3f}, Average F1: {avg_f1:.3f}\")\n",
        "\n",
        "    return total_loss / len(loader), accuracies"
      ],
      "metadata": {
        "id": "a1MubPh4lfUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following two cells are commented out because, to speed up training, we have already trained the model and uploaded the weights to Drive."
      ],
      "metadata": {
        "id": "AoWf_R_ipjoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "# batch_size = 64\n",
        "# epochs = 5\n",
        "# device = \"cuda\"\n",
        "\n",
        "# Optimizer\n",
        "# optimizer = Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# training\n",
        "# train(model, tokenized_train, tokenized_val, optimizer, batch_size, epochs, device)"
      ],
      "metadata": {
        "id": "J0J_CFyyqNrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# save_path = \"/content/drive/MyDrive/Colab_Notebooks\"\n",
        "\n",
        "# torch.save(model.state_dict(), f\"{save_path}/model_weights.pth\")"
      ],
      "metadata": {
        "id": "CXH_kcAKG6a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We download the weights of the trained model from Drive."
      ],
      "metadata": {
        "id": "6aS2nVZssw2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://drive.google.com/file/d/1b6o8qy81VB2DFzOVReaI46ZEx-9dDkJw/view?usp=drive_link\"\n",
        "\n",
        "downloaded_file = \"folder_PESI.zip\"\n",
        "\n",
        "# --- Download the file ---\n",
        "gdown.download(f\"https://drive.google.com/uc?id={\"1b6o8qy81VB2DFzOVReaI46ZEx-9dDkJw\"}\", downloaded_file, quiet=False)\n",
        "\n",
        "# --- extract the ZIP file ---\n",
        "output_folder = \"folder_PESI\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "with zipfile.ZipFile(downloaded_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_folder)\n",
        "\n",
        "subfolder = os.path.join(\"folder_PESI\", \"PESI\")\n",
        "files = os.listdir(subfolder)\n",
        "\n",
        "print(f\"Download completed and folder extracted to '{output_folder}'\")\n",
        "print(\"Contents of folder_PESI:\", os.listdir(output_folder))\n",
        "print(\"Contents of PESI:\",files)"
      ],
      "metadata": {
        "id": "uiktaYCwX3ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"folder_PESI/PESI/model_weights.pth\", map_location=torch.device(\"cpu\")))"
      ],
      "metadata": {
        "id": "59B8Qj0HZ30y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now test the model on unsee data."
      ],
      "metadata": {
        "id": "qC29rIzgs3Ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(tokenized_test, batch_size=64, shuffle=False)\n",
        "print(\"--- Test on unseen data ---\")\n",
        "validation_loss, val_accuracies = evaluate(model,test_loader,device=\"cuda\")"
      ],
      "metadata": {
        "id": "f3pvmMg1aMsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Result visualization**"
      ],
      "metadata": {
        "id": "Dbgts-CbqsJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will plot one graph per model showing precision, recall, and F1-score for the six tags in the test set."
      ],
      "metadata": {
        "id": "L22bfzHTuI4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# First Model\n",
        "tags_A = ['BadEnding', 'Conflict', 'Dialogue', 'Foreshadowing', 'MoralValue', 'Twist']\n",
        "precision_A = [0.7213, 0.2845, 0.9156, 0.2961, 0.7118, 0.7755]\n",
        "recall_A = [0.9610, 0.7993, 0.8985, 0.7721, 0.9525, 0.9022]\n",
        "f1_A = [0.8241, 0.4197, 0.9070, 0.4281, 0.8148, 0.8340]\n",
        "\n",
        "# Second Model\n",
        "tags_B = ['BadEnding', 'Conflict', 'Dialogue', 'Foreshadowing', 'MoralValue', 'Twist']\n",
        "precision_B = [0.928, 0.593, 0.889, 0.599, 0.832, 0.888]\n",
        "recall_B = [0.843, 0.362, 0.959, 0.309, 0.831, 0.849]\n",
        "f1_B = [0.883, 0.450, 0.922, 0.407, 0.832, 0.868]\n",
        "\n",
        "# Third Model\n",
        "tags_C = ['BadEnding', 'Conflict', 'Dialogue', 'Foreshadowing', 'MoralValue', 'Twist']\n",
        "precision_C = [0.955, 0.688, 0.902, 0.517, 0.902, 0.786]\n",
        "recall_C = [0.893, 0.495, 0.970, 0.553, 0.839, 0.951]\n",
        "f1_C = [0.923, 0.576, 0.934, 0.534, 0.869, 0.860]\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(3, 1, figsize=(10,6))\n",
        "\n",
        "# First Model\n",
        "x = np.arange(len(tags_A))\n",
        "width = 0.25\n",
        "axes[0].bar(x - width, precision_A, width, label='Precision')\n",
        "axes[0].bar(x, recall_A, width, label='Recall')\n",
        "axes[0].bar(x + width, f1_A, width, label='F1-score')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels(tags_A)\n",
        "axes[0].set_ylabel('Metric Value')\n",
        "axes[0].set_ylim(0, 1)\n",
        "axes[0].set_title('Tag-level Metrics for the First Model (Test Set)')\n",
        "axes[0].legend()\n",
        "\n",
        "# Second model\n",
        "x = np.arange(len(tags_B))\n",
        "axes[1].bar(x - width, precision_B, width, label='Precision')\n",
        "axes[1].bar(x, recall_B, width, label='Recall')\n",
        "axes[1].bar(x + width, f1_B, width, label='F1-score')\n",
        "axes[1].set_xticks(x)\n",
        "axes[1].set_xticklabels(tags_B)\n",
        "axes[1].set_ylabel('Metric Value')\n",
        "axes[1].set_ylim(0, 1)\n",
        "axes[1].set_title('Label-level Metrics for the Second Model (Test set)')\n",
        "axes[1].legend()\n",
        "\n",
        "# Third model \n",
        "x = np.arange(len(tags_C))\n",
        "axes[2].bar(x - width, precision_C, width, label='Precision')\n",
        "axes[2].bar(x, recall_C, width, label='Recall')\n",
        "axes[2].bar(x + width, f1_C, width, label='F1-score')\n",
        "axes[2].set_xticks(x)\n",
        "axes[2].set_xticklabels(tags_C)\n",
        "axes[2].set_ylabel('Metric Value')\n",
        "axes[2].set_ylim(0, 1)\n",
        "axes[2].set_title('Label-level Metrics for the Third Model (Test set)')\n",
        "axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bK1FnKrlpe0R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
